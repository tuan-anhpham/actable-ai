{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define functions for evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import properscoring as ps\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from math import sqrt\n",
    "\n",
    "\n",
    "def mean_absolute_percentage_error(actual, pred): \n",
    "    actual, pred = np.array(actual), np.array(pred)\n",
    "    return np.mean(np.abs((actual - pred) / actual)) * 100\n",
    "\n",
    "def root_mean_square_error(actual, pred):\n",
    "    actual, pred = np.array(actual), np.array(pred)\n",
    "    mse = mean_squared_error(actual, pred)\n",
    "    rmse = sqrt(mse)\n",
    "    return rmse\n",
    "\n",
    "def quantile_loss(q, actual, pred):\n",
    "    actual, pred = np.array(actual), np.array(pred)\n",
    "    \n",
    "    errors = pred - actual\n",
    "    over_pred = q * errors\n",
    "    under_pred = (q-1) * errors\n",
    "    max_errors = np.maximum(over_pred, under_pred)\n",
    "    loss = np.mean(max_errors)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def continuous_ranked_probability_score(actual, pred):\n",
    "    crps = np.mean(ps.crps_ensemble(actual, pred))\n",
    "    return crps\n",
    "\n",
    "def evaluation_metrics(actual, predicted):\n",
    "    rmse = root_mean_square_error(actual, predicted)\n",
    "    mae = mean_absolute_error(actual, predicted)\n",
    "    mape = mean_absolute_percentage_error(actual, predicted)\n",
    "    quantile_loss_01 = quantile_loss(0.1, actual, predicted)\n",
    "    quantile_loss_05 = quantile_loss(0.5, actual, predicted)\n",
    "    quantile_loss_095 = quantile_loss(0.95, actual, predicted)\n",
    "    crps = continuous_ranked_probability_score(actual, predicted)\n",
    "    \n",
    "    metric_dict = {'RMSE':rmse, 'MAE':mae, 'MAPE':mape, 'QuantileLoss[0.1]':quantile_loss_01, \\\n",
    "                   'QuantileLoss[0.5]':quantile_loss_05, 'QuantileLoss[0.95]':quantile_loss_095, 'CRPS':crps}\n",
    "    return metric_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An analysis of time-series forecasting models on daily_bike_sharing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "daily_bike = pd.read_csv(\"daily_bike_sharing_one_variable.csv\", encoding='latin-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation on hum variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data for evaluating hum variable\n",
    "actual_hum = daily_bike['hum'][-40:].values\n",
    "tableau_hum = daily_bike['hum_tableau'][-40:].values\n",
    "actableai_hum = daily_bike['hum_actableai'][-40:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    ActableAI    Tableau\n",
      "RMSE                 0.190429   0.190486\n",
      "MAE                  0.146373   0.167938\n",
      "MAPE                23.192907  31.190225\n",
      "QuantileLoss[0.1]    0.090496   0.028378\n",
      "QuantileLoss[0.5]    0.073187   0.083969\n",
      "QuantileLoss[0.95]   0.053713   0.146508\n",
      "CRPS                 0.146373   0.167938\n"
     ]
    }
   ],
   "source": [
    "# Compare the performance of time-series forecasting model in ActableAI and Tableau\n",
    "hum_comparison_df = pd.DataFrame({'ActableAI':pd.Series(evaluation_metrics(actual_hum, actableai_hum)), \\\n",
    "                             'Tableau':pd.Series(evaluation_metrics(actual_hum, tableau_hum))})\n",
    "print(hum_comparison_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation on casual variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data for evaluating casual variable\n",
    "actual_casual = daily_bike['casual'][-40:].values\n",
    "tableau_casual = daily_bike['casual_tableau'][-40:].values\n",
    "actableai_casual = daily_bike['casual_actableai'][-40:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     ActableAI     Tableau\n",
      "RMSE                443.246235  646.180277\n",
      "MAE                 297.275000  484.900000\n",
      "MAPE                 59.295510   89.577755\n",
      "QuantileLoss[0.1]   119.387500  105.930000\n",
      "QuantileLoss[0.5]   148.637500  242.450000\n",
      "QuantileLoss[0.95]  181.543750  396.035000\n",
      "CRPS                297.275000  484.900000\n"
     ]
    }
   ],
   "source": [
    "# Compare the performance of time-series forecasting model in ActableAI and Tableau\n",
    "casual_comparison_df = pd.DataFrame({'ActableAI':pd.Series(evaluation_metrics(actual_casual, actableai_casual)), \\\n",
    "                             'Tableau':pd.Series(evaluation_metrics(actual_casual, tableau_casual))})\n",
    "print(casual_comparison_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An analysis of time-series forecasting models on jane_empties dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "jane_empties = pd.read_csv(\"Jane_empties_series_one_variable.csv\", encoding='latin-1')\n",
    "jane_test_data = jane_empties[-30:]\n",
    "jane_test_data = jane_test_data.dropna(axis='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation on return_customers variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data for evaluating return_customers variable\n",
    "actual_return_customers = jane_test_data['return_customers'].values\n",
    "tableau_return_customers = jane_test_data['return_customer_tableau'].values\n",
    "actableai_return_customers = jane_test_data['return_customers_actableai'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    ActableAI    Tableau\n",
      "RMSE                 3.739435   3.332184\n",
      "MAE                  2.567302   2.551724\n",
      "MAPE                31.229668  37.553431\n",
      "QuantileLoss[0.1]    1.996458   1.744828\n",
      "QuantileLoss[0.5]    1.283651   1.275862\n",
      "QuantileLoss[0.95]   0.481743   0.748276\n",
      "CRPS                 2.567302   2.551724\n"
     ]
    }
   ],
   "source": [
    "# Compare the performance of time-series forecasting model in ActableAI and Tableau\n",
    "return_customers_comparison_df = pd.DataFrame({'ActableAI':pd.Series(evaluation_metrics(actual_return_customers, actableai_return_customers)), \\\n",
    "                             'Tableau':pd.Series(evaluation_metrics(actual_return_customers, tableau_return_customers))})\n",
    "print(return_customers_comparison_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation on new_customers variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data for evaluating new_customers variable\n",
    "actual_new_customers = jane_test_data['new_customers'].values\n",
    "tableau_new_customers = jane_test_data['new_customers_tableau'].values\n",
    "actableai_new_customers = jane_test_data['new_customers_actableai'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    ActableAI    Tableau\n",
      "RMSE                 2.983155   3.000000\n",
      "MAE                  2.567707   2.586207\n",
      "MAPE                68.590778  83.647186\n",
      "QuantileLoss[0.1]    1.349023   1.058621\n",
      "QuantileLoss[0.5]    1.283853   1.293103\n",
      "QuantileLoss[0.95]   1.210537   1.556897\n",
      "CRPS                 2.567707   2.586207\n"
     ]
    }
   ],
   "source": [
    "# Compare the performance of time-series forecasting model in ActableAI and Tableau\n",
    "new_customers_comparison_df = pd.DataFrame({'ActableAI':pd.Series(evaluation_metrics(actual_new_customers, actableai_new_customers)), \\\n",
    "                             'Tableau':pd.Series(evaluation_metrics(actual_new_customers, tableau_new_customers))})\n",
    "print(new_customers_comparison_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
